{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-Up APIs, Dependencies, Function Definitions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from credentials import *\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import re\n",
    "import tweepy\n",
    "import spacy\n",
    "import sqlalchemy as db\n",
    "from config import *\n",
    "import json\n",
    "import pickle\n",
    "from country_list import countries_for_language\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_setup():\n",
    "    \"\"\"Set up Tweepy. \n",
    "    \n",
    "    This function borrowed from Rodolfo Ferro, \n",
    "    https://dev.to/rodolfoferro/sentiment-analysis-on-trumpss-tweets-using-python-\"\"\"\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "    \n",
    "    api = tweepy.API(auth)\n",
    "    return api\n",
    "\n",
    "tweet_catcher = twitter_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidate_tweets(handle, pagenum): #page 1 is first\n",
    "    \"\"\"\n",
    "    Download tweets using Tweepy wrapper for Twitter API.\n",
    "    \n",
    "    Params:\n",
    "    handle -- string, the target user's Twitter handle\n",
    "    pagenum -- int, the cursor pagination\n",
    "    returns: a list of Tweepy.Tweet objects\"\"\"\n",
    "    tweetlist = tweet_catcher.user_timeline(screen_name=handle, count=200, page=str(pagenum), tweet_mode=\"extended\")\n",
    "    return tweetlist\n",
    "\n",
    "def get_base_tweets(handlelist):\n",
    "    \"\"\"Download a sample set of tweets (approx. 600) for each handle in a list.\n",
    "    \n",
    "    Param:\n",
    "    handlelist -- list of strings containing the handles of Twitter accounts to download.\n",
    "    returns: a list of JSON-format tweet objects.\n",
    "    \"\"\"\n",
    "    tweetlist = []\n",
    "    for handle in handlelist:\n",
    "        candidate_tweets = []\n",
    "        page = 1\n",
    "        while page < 2:\n",
    "            tweets = get_candidate_tweets(handle, str(page))\n",
    "            page += 1\n",
    "            for tweet in tweets:\n",
    "                candidate_tweets.append(tweet._json)\n",
    "        for tweet in candidate_tweets:\n",
    "            tweetlist.append(tweet)\n",
    "    return tweetlist\n",
    "\n",
    "def filter_quoted_tweet(tweet):\n",
    "    \"\"\"Return the original quoted tweet object.\n",
    "    \n",
    "    Param:\n",
    "    tweet -- a JSON-format tweet object\n",
    "    returns: None if not a quoted tweet, otherwise the original tweet object\n",
    "    from which the passed tweet was quoted.\"\"\"\n",
    "    if \"quoted_status\" in tweet:\n",
    "        return tweet[\"quoted_status\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def filter_retweet(tweet):\n",
    "    \"\"\"Return the original of a retweeted tweet.\n",
    "    \n",
    "    Param:\n",
    "    tweet -- a JSON-format tweet object\n",
    "    returns: None if not a retweet, otherwise the original tweet object\n",
    "    from which the passed tweet was retweeted.\"\"\"\n",
    "    if \"retweeted_status\" in tweet:\n",
    "        return tweet[\"retweeted_status\"]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "#use tweet[\"full_text\"]\n",
    "def clean_tweet(tweet_text):\n",
    "    \"\"\"Prepare a tweet for natural language processing.\n",
    "    \n",
    "    Param:\n",
    "    tweet -- the full text of a tweet\n",
    "    returns: a string, the tweet's text after formatting.\n",
    "    \"\"\"\n",
    "    tweet_text = re.sub(r'\\S+\\.\\S+\\/\\S+', \"\", tweet_text) #remove links\n",
    "    tweet_text = re.sub(r'@([A-Za-z0-9]+)', r'\\1', tweet_text) #replace \"@username\" with \"username\"\n",
    "    while \"#\" in tweet_text:\n",
    "        tweet_text = re.sub(r'#(.+)', r'\\1', tweet_text) #replace \"#hashtag\" with \"hashtag\"\n",
    "    tweet_text = re.sub(r'\\s(rt)|(RT)', '', tweet_text) #remove \"RT\" indicating retweet\n",
    "    tweet_text = tweet_text.replace(\"\\n\", \" \") # remove \\n characters\n",
    "    tweet_text = re.sub(r'[^\\u0000-\\u0080]', \"\", tweet_text) #remove non-ascii utf8 chars (ie, emojis)\n",
    "    html_issues = {\"&amp\" : \"and\",\n",
    "                   \"&gt\" : \">\",\n",
    "                   \"&lt\" : \"<\",\n",
    "                  }\n",
    "    for prob, sol in html_issues.items():\n",
    "        tweet_text = tweet_text.replace(prob, sol)\n",
    "    tweet_text = tweet_text.lower()\n",
    "    return tweet_text\n",
    "\n",
    "def process_texts(nlplist):\n",
    "    \"\"\"Use SpaCy API to remove stopwords, numerals, and whitespaces \n",
    "    from a list of tweet texts.\n",
    "    \n",
    "    Param:\n",
    "    nlplist -- a list of SpaCy text objects.\n",
    "    \"\"\"\n",
    "    processed_texts = []\n",
    "    for tweet in nlplist:\n",
    "        ptweet = remove_tokens(tweet)\n",
    "        processed_texts.append(ptweet)\n",
    "    return processed_texts\n",
    "\n",
    "def remove_tokens(onenlptweet):\n",
    "    \"\"\"Helper function to process_texts(). Remove stopwords, spaces, and numerals.\n",
    "    \n",
    "    Param:\n",
    "    onenlptweet -- a SpaCy text object tokenized and POS-tagged.\n",
    "    \"\"\"\n",
    "    donetweet = []\n",
    "    for token in onenlptweet:\n",
    "        if not token.is_stop and not token.is_punct and not token.like_num and not token.is_space:\n",
    "            donetweet.append(token.lemma_)\n",
    "    return donetweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def get_nyt_hits(page, start_date, end_date):\n",
    "    \"\"\"Display the number of hits in a New York Times article search API call \n",
    "    for a given date range.\n",
    "    \n",
    "    Params:\n",
    "    page -- int between 0 and 100, the response cursor's pagination.\n",
    "    start_date -- string, begin date of the date range in YYYYMMDD format\n",
    "    end_date -- string, end date of the date range in YYYYMMDD format\n",
    "    \"\"\"\n",
    "    r1 = requests.get(f'https://api.nytimes.com/svc/search/v2/articlesearch.json?fq=news_desk:(\"National\" \"Politics\" \"U.S.\")&begin_date={start_date}&end_date={end_date}&page={page}&sort=oldest&api-key={NYT_KEY}')\n",
    "    print(\"hits: \", r1.json()[\"response\"][\"meta\"][\"hits\"])\n",
    "\n",
    "def get_all_articles(page, start_date, end_date):\n",
    "    \"\"\"Get article data from New York Times article search API call for given date range.\n",
    "    \n",
    "    Params:\n",
    "    page -- int between 0 and 100, response cursor's pagination.\n",
    "    start_date -- string, begin date of the date range in YYYYMMDD format\n",
    "    end_date -- string, end date of the date range in YYYYMMDD format\n",
    "    returns: list of JSON objects containing article metadata including headlines and snippets.\n",
    "    \"\"\"\n",
    "    r1 = requests.get(f'https://api.nytimes.com/svc/search/v2/articlesearch.json?fq=news_desk:(\"National\" \"Politics\" \"U.S.\")&begin_date={start_date}&end_date={end_date}&page={page}&sort=oldest&api-key={NYT_KEY}')\n",
    "    articles = [obj for obj in r1.json()[\"response\"][\"docs\"]]\n",
    "    while page < 100:\n",
    "        page += 1\n",
    "        r2 = requests.get(f'https://api.nytimes.com/svc/search/v2/articlesearch.json?fq=news_desk:(\"National\" \"Politics\" \"U.S.\")&begin_date={start_date}&end_date={end_date}&page={page}&sort=oldest&api-key={NYT_KEY}')\n",
    "        newarticles = [obj for obj in r2.json()[\"response\"][\"docs\"]]\n",
    "        for item in newarticles:\n",
    "            articles.append(item)\n",
    "        time.sleep(6)\n",
    "    return articles\n",
    "\n",
    "#(obj[\"headline\"][\"main\"].lower(), obj[\"snippet\"].lower(), obj[\"pub_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(f'mysql+mysqldb://{user}:{pw}@{host}/trailtracker?charset=utf8mb4', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = engine.connect()\n",
    "meta = db.MetaData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Base Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = [\"CoryBooker\", \"PeteButtigieg\", \"JulianCastro\", \"JohnDelaney\", \"TulsiGabbard\"] \n",
    "h2 = [\"SenGillibrand\", \"KamalaHarris\", \"Hickenlooper\", \"JayInslee\", \"amyklobuchar\"] \n",
    "h3 = [\"BetoORourke\", \"ewarren\", \"marwilliamson\", \"AndrewYang\", \"realDonaldTrump\"]\n",
    "h4 = [\"SenSanders\", \"TimRyan\", \"MikeGravel\", \"ericswalwell\", \"WayneMessam\"]\n",
    "h5 = [\"sethmoulton\", \"GovBillWeld\", \"JoeBiden\", \"BilldeBlasio\", \"GovernorBullock\", \"MichaelBennet\"] \n",
    "allhandles = h1 + h2 + h3 + h4 + h5\n",
    "candidate_fn = [\"Cory\", \"Pete\", \"Julian\", \"John\", \"Tulsi\", \"Kirsten\", \"Kamala\", \"John\", \"Jay\", \"Amy\", \"Beto\", \"Elizabeth\", \"Marianne\", \"Andrew\", \"Donald\", \"Bernie\", \"Tim\", \"Mike\", \"Eric\", \"Wayne\", \"Seth\", \"Bill\", \"Joe\", \"Bill\", \"Steve\", \"Michael\"]\n",
    "candidate_ln = [\"Booker\", \"Buttigieg\", \"Castro\", \"Delaney\", \"Gabbard\", \"Gillibrand\", \"Harris\", \"Hickenlooper\", \"Inslee\", \"Klobuchar\", \"O'Rourke\", \"Warren\", \"Williamson\", \"Yang\", \"Trump\", \"Sanders\", \"Ryan\", \"Gravel\", \"Swalwell\", \"Messam\", \"Moulton\", \"Weld\", \"Biden\", \"de Blasio\", \"Bullock\", \"Bennet\"]\n",
    "date_announced = [\"2019-02-01\", \"2019-01-23\", \"2019-01-12\", \"2017-07-28\", \"2019-01-11\", \"2019-01-15\", \"2019-01-21\", \"2019-03-04\", \"2019-03-01\", \"2019-02-10\", \"2019-03-14\", \"2018-12-31\", \"2019-01-28\", \"2018-11-06\", None, \"2019-02-19\", \"2019-04-04\", \"2019-04-04\", \"2019-04-08\", \"2019-03-28\", \"2019-04-22\", \"2019-04-15\", \"2019-04-25\", \"2019-05-16\", \"2019-05-14\", \"2019-05-02\"]\n",
    "len(date_announced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#200 most recent tweets per candidate\n",
    "l1 = get_base_tweets(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = get_base_tweets(h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3 = get_base_tweets(h3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4 = get_base_tweets(h4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l5 = get_base_tweets(h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_tweets = l1 + l2 + l3 + l4 + l5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collected_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pickling tweet objects as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileout = open(\"tweetsaver\", \"wb\")\n",
    "pickle.dump(collected_tweets, fileout)\n",
    "fileout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filein = open(\"tweetsaver\", \"rb\")\n",
    "collected_tweets = pickle.load(filein)\n",
    "filein.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collected_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out retweets and quoted tweets for database processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rts = [filter_retweet(tweet) for tweet in collected_tweets]\n",
    "qts = [filter_quoted_tweet(tweet) for tweet in collected_tweets]\n",
    "qts2 = [tweet for tweet in qts if tweet != None]\n",
    "rts2 = [tweet for tweet in rts if tweet != None]\n",
    "qrts = rts2 + qts2\n",
    "tweets_noqrts = [tweet for tweet in collected_tweets if \"quoted_status\" not in tweet and \"retweeted_status\" not in tweet]\n",
    "tweets_combined = tweets_noqrts + qrts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect NYT Headlines and Article Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits:  871\n"
     ]
    }
   ],
   "source": [
    "get_nyt_hits(0, 20181106, 20190201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles1 = get_all_articles(0, 20181106, 20190201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits:  825\n"
     ]
    }
   ],
   "source": [
    "get_nyt_hits(0, 20190202, 20190501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles2 = get_all_articles(0, 20190202, 20190501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "allarticles = articles1 + articles2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_fileout = open(\"articlesaver\", \"wb\")\n",
    "pickle.dump(allarticles, nyt_fileout)\n",
    "nyt_fileout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_filein = open(\"articlesaver\", \"rb\")\n",
    "allarticles = pickle.load(nyt_filein)\n",
    "nyt_filein.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_texts = [clean_tweet(tweet[\"full_text\"]) for tweet in rts if tweet]\n",
    "qt_texts = [clean_tweet(tweet[\"full_text\"]) for tweet in qts if tweet]\n",
    "tweet_texts = [clean_tweet(tweet[\"full_text\"]) for tweet in collected_tweets if \"quoted_status\" not in tweet and \"retweeted_status\" not in tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_headlines = [obj[\"headline\"][\"main\"].lower() for obj in allarticles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_descriptions = [obj[\"snippet\"].lower() for obj in allarticles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8592"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts = rt_texts + qt_texts + tweet_texts + nyt_headlines + nyt_descriptions\n",
    "tweet_texts_combined = tweet_texts + rt_texts + qt_texts\n",
    "len(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_nlp = [nlp(text) for text in all_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = process_texts(texts_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phrases(processed_texts)\n",
    "bigrammed_texts = [bigram[text] for text in processed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = Phrases(bigrammed_texts)\n",
    "trigrammed_texts = [trigram[text] for text in bigrammed_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_nlp = [nlp(tweet) for tweet in tweet_texts_combined]\n",
    "articles_nlp = [nlp(headline) for headline in nyt_headlines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive keyword classification of tweets into topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_keywords = {\n",
    "#     \"Immigration\" : [\"immigration\", \"immigrant\", \"border\", \"migrant\", \"asylum\"],\n",
    "#     \"Taxes\": [\"tax\", \"taxation\", \" irs \"],\n",
    "#     \"Economy\": [\"economy\", \"economic\", \"recession\", \"gdp\", \"income\", \"inflation\", \"monetary\"],\n",
    "#     \"Racism\": [\"race\", \"racism\", \"racist\", \"anti-black\", \"white supremacy\", \"supremacist\", \"hate crime\", \"minority\", \"hatred\", \"reparations\"],\n",
    "#     \"Foreign Policy\": [\"state department\", \"foreign policy\", \"china\", \"russia\", \"brazil\", \"india\", \"nato\", \"europe\", \"asia\", \"africa\", \"south africa\", \"south america\", \"mexico\", \"canada\", \"syria\", \"afghanistan\", \"iraq\", \"turkey\", \"israel\", \"egypt\", \"korea\", \"japan\", \"germany\", \"england\", \"eu\", \"uk\", \"france\"],\n",
    "#     \"Poverty\": [\"socioeconomic\", \"class\", \"the rich\", \"poor\", \"poverty\", \"low-income\", \"working people\"],\n",
    "#     \"Terrorism\": [\"terror\", \"terrorist\", \"terrorism\", \"isil\", \"isis\", \"al-qaeda\", \"daesh\", \"al-shabab\", \"islamic state\", \"bombing\", \"dhs\"],\n",
    "#     \"Guns\": [\"gun\", \"nra\", \"bump stock\", \"shooting\"],\n",
    "#     \"Education\": [\"education\", \"school\", \"curriculum\"],\n",
    "#     \"Health Care\": [\"medicare for all\", \"health care\", \"healthcare\", \"obamacare\", \"trumpcare\", \"aca\", \"health exchange\", \"single-payer\", \"public option\", \"insurance\", \"uninsured\", \"preexisting condition\"],\n",
    "#     \"Environment\": [\"environment\", \"green new deal\", \"climate change\", \"global warming\", \"carbon\", \"emissions\", \"pollution\", \"water supply\", \"environmental\"],\n",
    "#     \"LGBTQ+\": [\"lgbt\", \"lgbtq\", \"gay\", \"lesbian\", \"bisexual\", \"transgender\", \"gender identity\", \"trans\", \"cis\", \"heterosexism\"],\n",
    "#     \"Sexism\": [\"sexism\", \"patriarchy\", \"gender\"],\n",
    "#     \"Social Security\": [\"social security\", \"retirement\", \"pension\", \"ira\"],\n",
    "#     \"Employment\": [\"employment\", \"jobs\", \"working\", \"work\", \"unemployment\", \"unemployed\"],\n",
    "#     \"Refugees\": [\"refugee\", \"displaced person\", \"displaced people\"],\n",
    "#     \"Religion\": [\"religion\", \"faith\", \"god\", \"christianity\", \"judaism\", \"islam\", \"christian\", \"jew\", \"jewish\", \"muslim\", \"hinduism\", \"hindu\", \"buddhism\", \"buddhist\", \"atheist\", \"atheism\", \"humanism\", \"humanist\", \"secular\"],\n",
    "#     \"Drugs\": [\"drug\", \"marijuana\", \"opiod\", \"heroin\", \"cocaine\", \" dea \"],\n",
    "#     \"Policing\": [\"police\", \"prison\", \"law enforcement\", \"policing\"]\n",
    "# }\n",
    "\n",
    "countries = [re.compile(f'{country[1]}'.lower()) for country in countries_for_language('en')]\n",
    "fp_words = [re.compile(r'state department'), re.compile(r'foreign policy'), re.compile(r'\\bnato\\b'), re.compile(r'european union'), re.compile(r'\\beu\\b'), re.compile(r'african union'), re.compile(r'\\bau\\b'), re.compile(r'african?'), re.compile(r'pentagon'), re.compile(r'\\bwar\\b')]\n",
    "\n",
    "for word in fp_words:\n",
    "    countries.append(word)\n",
    "\n",
    "topics_keywords = {\n",
    "    \"Immigration\" : [re.compile(r'immigr.*'), re.compile(r'borders?'), re.compile(r'migrants?'), re.compile(r'asylum')],\n",
    "    \"Taxes\": [re.compile(r'\\btax(es)?\\b'), re.compile(r'\\birs\\b')],\n",
    "    \"Economy\": [re.compile(r'\\beconom.*\\b'), re.compile(r'recessions?'), re.compile(r'\\bgdp\\b'), re.compile(r'incomes?'), re.compile(r'inflation'), re.compile(r'monetary')],\n",
    "    \"Racism\": [re.compile(r'\\brace\\b'), re.compile(r'racis[t|m]s?'), re.compile(r'anti-black'), re.compile(r'white suprem(acy|acist)s?'), re.compile(r'hate crimes?'), re.compile(r'minorit(ies|y)'), re.compile(r'\\breparations\\b')],\n",
    "    \"Foreign Policy\": countries,\n",
    "    \"Poverty\": [re.compile(r'socioeconomic'), re.compile(r'the rich'), re.compile(r'poor'), re.compile(r'poverty'), re.compile(r'low-?\\s?income'), re.compile(r'working-?\\s?(people|class)')],\n",
    "    \"Terrorism\": [re.compile(r'terror(ist|ism)?s?'), re.compile(r'\\bisi(l|s)\\b'), re.compile(r'al-qaeda'), re.compile(r'daesh'), re.compile(r'al-shabab'), re.compile(r'islamic\\sstate'), re.compile(r'bomb(ing)?s?'), re.compile(r'\\bdhs\\b'), re.compile(r'homeland\\ssecurity')],\n",
    "    \"Guns\": [re.compile(r'guns?'), re.compile(r'\\bnra\\b'), re.compile(r'bump\\sstock'), re.compile(r'shoo?t(ing)?s?')],\n",
    "    \"Education\": [re.compile(r'education'), re.compile(r'schools?'), re.compile(r'curricul(um|a)?s?')],\n",
    "    \"Health Care\": [re.compile(r'medicare'), re.compile(r'medicaid'), re.compile(r'(health|obama|trump)-?\\s?care'), re.compile(r'\\baca\\b'), re.compile(r'affordable\\scare\\sact'), re.compile(r'single-?\\s?payer'), re.compile(r'public\\soption'), re.compile(r'(un)?insur(ed|ance)'), \"uninsured\", \"preexisting condition\"],\n",
    "    \"Environment\": [re.compile(r'environment(al)?(ism)?'), re.compile(r'green\\snew\\sdeal'), re.compile(r'climate\\schange'), re.compile(r'global\\swarming'), re.compile(r'\\bcarbon\\b'), re.compile(r'\\bemissions?\\b'), re.compile(r'pollution'), re.compile(r'water\\ssupply')],\n",
    "    \"LGBTQ+\": [re.compile(r'lgbtq?'), re.compile(r'gay'), re.compile(r'lesbian'), re.compile(r'\\bbi(sexual)?\\b'), re.compile(r'\\btrans(gender)?\\b'), re.compile(r'gender\\sidentity'), re.compile(r'\\btrans(gender)?\\b'), re.compile(r'\\bcis(gender)?\\b'), re.compile(r'heterosex')],\n",
    "    \"Sexism\": [re.compile(r'\\bsexism\\b'), re.compile(r'(pay)|(wage)\\sgap'), re.compile(r'patriarch')],\n",
    "    \"Social Security\": [re.compile(r'social\\ssecurity'), re.compile(r'\\bretired?(ment)?\\b'), re.compile(r'\\bpension(er)?\\b'), re.compile(r'\\bira\\b')],\n",
    "    \"Employment\": [re.compile(r'\\b(un)?(under)?-?employ(ed)?(ment)?(er)?s?\\b'), re.compile(r'\\bjobs?\\b'), re.compile(r'\\bworks?(ing)?(ed)?\\b')],\n",
    "    \"Refugees\": [re.compile(r'refugee'), re.compile(r'displaced\\s(person)|(people)s?')],\n",
    "    \"Religion\": [re.compile(r'religio'), re.compile(r'faith'), re.compile(r'god'), re.compile(r'christian'), re.compile(r'judaism'), re.compile(r'islam'), re.compile(r'jew'), re.compile(r'muslim'), re.compile(r'hindu'), re.compile(r'buddha?'), re.compile(r'atheis'), re.compile(r'humanis'), re.compile(r'secular')],\n",
    "    \"Drugs\": [\"drug\", \"marijuana\", \"opiod\", \"heroin\", \"cocaine\", \" dea \"],\n",
    "    \"Policing\": [\"police\", \"prison\", \"law enforcement\", \"policing\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_sort(tweetlist, topic_hash, text_hash):\n",
    "    \"\"\"Classify tweets by topic keywords.\n",
    "    \n",
    "    Params:\n",
    "    tweetlist -- list of JSON-format tweet objects\n",
    "    topic_hash -- dictionary with keys as topics, values as lists of keywords\n",
    "        (regex for better performance than totally-naive keywords)\n",
    "    text_hash -- dictionary with keys as topics matching those of topic_hash,\n",
    "        values as empty lists.\n",
    "    returns: dictionary with keys as topics, values as lists of strings containing texts.\n",
    "    \"\"\"\n",
    "    for tweet in tweetlist:\n",
    "        for kvpair in topic_hash.items():\n",
    "            for keyword in kvpair[1]:\n",
    "                if keyword in clean_tweet(tweet[\"full_text\"]):\n",
    "                    text_hash[kvpair[0]].append(tweet)\n",
    "    return text_hash\n",
    "\n",
    "def article_sort(text_list, topic_hash, text_hash):\n",
    "    \"\"\"\n",
    "    Classify headline texts by topic keywords.\n",
    "    \n",
    "    Params:\n",
    "    text_list -- list of text of article headlines and/or snippets.\n",
    "    topic_hash -- dictionary with keys as topics, values as lists of keywords\n",
    "        (regex for better performance than totally-naive keywords)\n",
    "    text_hash -- dictionary with keys as topics matching those of topic_hash,\n",
    "        values as empty lists.\n",
    "    returns: dictionary with keys as topics, values as lists of strings \n",
    "        (the sorted article texts).\n",
    "    \"\"\"\n",
    "    for doc in text_list:\n",
    "        for kvpair in topic_hash.items():\n",
    "            for keyword in kvpair[1]:\n",
    "                if keyword in doc:\n",
    "                    text_hash[kvpair[0]].append(doc)\n",
    "    return text_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_tweet_topics = {topic: [] for topic in topics_keywords.keys()}\n",
    "tweets_by_topic = tweet_sort(tweets_combined, topics_keywords, empty_tweet_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_article_topics = {topic: [] for topic in topics_keywords.keys()}\n",
    "articles_by_topic = article_sort(nyt_headlines, topics_keywords, empty_article_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Data for insert into MySQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame({\n",
    "    \"TopicNum\": [i for i in range(19)],\n",
    "    \"TopicName\": [kw for kw in topics_keywords.keys()]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopicNum</th>\n",
       "      <th>TopicName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Immigration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Racism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Foreign Policy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TopicNum       TopicName\n",
       "0         0     Immigration\n",
       "1         1           Taxes\n",
       "2         2         Economy\n",
       "3         3          Racism\n",
       "4         4  Foreign Policy"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df.to_sql(\"topic_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x20851ff8a20>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute(\"ALTER TABLE topic_t ADD CONSTRAINT Topic_PK PRIMARY KEY(TopicNum)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter_User_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "allusers = [tweet[\"user\"][\"name\"] for tweet in tweets_combined]\n",
    "unique_users = set(allusers)\n",
    "unique_users_list = list(unique_users)\n",
    "users_df = pd.DataFrame({\n",
    "    \"UserID\": [i for i, t in enumerate(unique_users_list)],\n",
    "    \"UserName\": [name for name in unique_users_list]\n",
    "})\n",
    "userdf2 = pd.DataFrame({\n",
    "    \"UserName\": [tweet[\"user\"][\"name\"] for tweet in tweets_combined]\n",
    "})\n",
    "userdf3 = userdf2.merge(users_df, how=\"left\", on=\"UserName\")\n",
    "twitter_users_df = pd.DataFrame({\n",
    "    \"UserID\": userdf3.UserID,\n",
    "    \"UserName\": [tweet[\"user\"][\"name\"] for tweet in tweets_combined],\n",
    "    \"TwitterHandle\": [tweet[\"user\"][\"screen_name\"] for tweet in tweets_combined]\n",
    "})\n",
    "twitter_users_df = twitter_users_df.drop_duplicates(subset=\"UserID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>UserName</th>\n",
       "      <th>TwitterHandle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>832</td>\n",
       "      <td>Cory Booker</td>\n",
       "      <td>CoryBooker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>219</td>\n",
       "      <td>Pete Buttigieg</td>\n",
       "      <td>PeteButtigieg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>904</td>\n",
       "      <td>Julián Castro</td>\n",
       "      <td>JulianCastro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>699</td>\n",
       "      <td>John Delaney</td>\n",
       "      <td>JohnDelaney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>306</td>\n",
       "      <td>Tulsi Gabbard</td>\n",
       "      <td>TulsiGabbard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     UserID        UserName  TwitterHandle\n",
       "0       832     Cory Booker     CoryBooker\n",
       "102     219  Pete Buttigieg  PeteButtigieg\n",
       "202     904   Julián Castro   JulianCastro\n",
       "289     699    John Delaney    JohnDelaney\n",
       "423     306   Tulsi Gabbard   TulsiGabbard"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_users_df.to_sql(\"twitter_user_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"ALTER TABLE twitter_user_t ADD CONSTRAINT Twitter_User_PK PRIMARY KEY(UserID)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_user_t = db.Table(\"twitter_user_t\", meta, autoload=True, autoload_with=engine)\n",
    "s = db.select([twitter_user_t])\n",
    "result = conn.execute(s)\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_df = pd.DataFrame({\n",
    "    \"TwitterHandle\": [handle for handle in allhandles]\n",
    "})\n",
    "cdf2 = cand_df.merge(twitter_users_df, how=\"left\", on=\"TwitterHandle\")\n",
    "cdf2 = cdf2.drop_duplicates()\n",
    "candidates_df = pd.DataFrame({\n",
    "    \"CandidateID\": cdf2.UserID,\n",
    "    \"CandidateLastName\": candidate_ln,\n",
    "    \"CandidateFirstName\": candidate_fn,\n",
    "    \"DateAnnounced\": date_announced,\n",
    "    \"Party\": [\"Democratic\" if name != \"Trump\" and name != \"Weld\" else \"Republican\" for name in candidate_ln]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CandidateID</th>\n",
       "      <th>CandidateLastName</th>\n",
       "      <th>CandidateFirstName</th>\n",
       "      <th>DateAnnounced</th>\n",
       "      <th>Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>832</td>\n",
       "      <td>Booker</td>\n",
       "      <td>Cory</td>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>219</td>\n",
       "      <td>Buttigieg</td>\n",
       "      <td>Pete</td>\n",
       "      <td>2019-01-23</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>904</td>\n",
       "      <td>Castro</td>\n",
       "      <td>Julian</td>\n",
       "      <td>2019-01-12</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>Delaney</td>\n",
       "      <td>John</td>\n",
       "      <td>2017-07-28</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>Gabbard</td>\n",
       "      <td>Tulsi</td>\n",
       "      <td>2019-01-11</td>\n",
       "      <td>Democratic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CandidateID CandidateLastName CandidateFirstName DateAnnounced       Party\n",
       "0          832            Booker               Cory    2019-02-01  Democratic\n",
       "1          219         Buttigieg               Pete    2019-01-23  Democratic\n",
       "2          904            Castro             Julian    2019-01-12  Democratic\n",
       "3          699           Delaney               John    2017-07-28  Democratic\n",
       "4          306           Gabbard              Tulsi    2019-01-11  Democratic"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_df.to_sql(\"candidate_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"ALTER TABLE candidate_t ADD CONSTRAINT Candidate_PK PRIMARY KEY(CandidateID)\")\n",
    "conn.execute(\"ALTER TABLE candidate_t ADD CONSTRAINT Candidate_FK FOREIGN KEY (CandidateID) REFERENCES Twitter_User_T(UserID)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame({\n",
    "    #\"TweetID\" : [i for i, tweet in enumerate(tweets_combined)],\n",
    "    \"TweetID\" : [tweet[\"id_str\"] for tweet in tweets_combined],\n",
    "    \"LengthWords\": [len(tweet) for tweet in tweets_nlp],\n",
    "    \"TweetFullText\": [tweet[\"full_text\"] for tweet in tweets_combined],\n",
    "    \"TweetDate\" : [time.strftime(\"%Y-%m-%d\", time.strptime(tweet[\"created_at\"],'%a %b %d %H:%M:%S +0000 %Y')) for tweet in tweets_combined],\n",
    "    \"TimesRetweeted\" : [tweet[\"retweet_count\"] for tweet in tweets_combined],\n",
    "    \"UserID\" : userdf3.UserID\n",
    "})\n",
    "tweets_df = tweets_df.drop_duplicates(subset=\"TweetID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetID</th>\n",
       "      <th>LengthWords</th>\n",
       "      <th>TweetFullText</th>\n",
       "      <th>TweetDate</th>\n",
       "      <th>TimesRetweeted</th>\n",
       "      <th>UserID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1132456883116285953</td>\n",
       "      <td>17</td>\n",
       "      <td>@RUSSBARNES Clearly I crack myself up too. Min...</td>\n",
       "      <td>2019-05-26</td>\n",
       "      <td>1</td>\n",
       "      <td>832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1132414330237014016</td>\n",
       "      <td>34</td>\n",
       "      <td>On the road in Iowa and the drives get long. S...</td>\n",
       "      <td>2019-05-25</td>\n",
       "      <td>507</td>\n",
       "      <td>832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1132332387763937280</td>\n",
       "      <td>40</td>\n",
       "      <td>The moment Joan told me she was going to caucu...</td>\n",
       "      <td>2019-05-25</td>\n",
       "      <td>79</td>\n",
       "      <td>832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1132280827713343488</td>\n",
       "      <td>19</td>\n",
       "      <td>@JennaSumar Are we even a family if we don't t...</td>\n",
       "      <td>2019-05-25</td>\n",
       "      <td>2</td>\n",
       "      <td>832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1132045252620169216</td>\n",
       "      <td>54</td>\n",
       "      <td>The abortion bans in Georgia, Alabama, Missour...</td>\n",
       "      <td>2019-05-24</td>\n",
       "      <td>152</td>\n",
       "      <td>832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TweetID  LengthWords  \\\n",
       "0  1132456883116285953           17   \n",
       "1  1132414330237014016           34   \n",
       "2  1132332387763937280           40   \n",
       "3  1132280827713343488           19   \n",
       "4  1132045252620169216           54   \n",
       "\n",
       "                                       TweetFullText   TweetDate  \\\n",
       "0  @RUSSBARNES Clearly I crack myself up too. Min...  2019-05-26   \n",
       "1  On the road in Iowa and the drives get long. S...  2019-05-25   \n",
       "2  The moment Joan told me she was going to caucu...  2019-05-25   \n",
       "3  @JennaSumar Are we even a family if we don't t...  2019-05-25   \n",
       "4  The abortion bans in Georgia, Alabama, Missour...  2019-05-24   \n",
       "\n",
       "   TimesRetweeted  UserID  \n",
       "0               1     832  \n",
       "1             507     832  \n",
       "2              79     832  \n",
       "3               2     832  \n",
       "4             152     832  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.to_sql(\"tweet_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"ALTER TABLE tweet_t ADD CONSTRAINT Tweet_PK PRIMARY KEY(TweetID)\")\n",
    "conn.execute(\"ALTER TABLE tweet_t ADD CONSTRAINT Tweet_FK_1 FOREIGN KEY(TweetDominantTopic) REFERENCES Topic_T(TopicNum)\")\n",
    "conn.execute(\"ALTER TABLE tweet_t ADD CONSTRAINT Tweet_FK_2 FOREIGN KEY(UserID) REFERENCES Twitter_User_T(UserID)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retweet_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsdf2 = tweets_df.drop_duplicates(subset=\"TweetFullText\")\n",
    "rtdf1 = pd.DataFrame({\n",
    "    \"TweetFullText\": [tweet[\"full_text\"] for tweet in qrts]\n",
    "})\n",
    "rtdf2 = rtdf1.merge(tweetsdf2, how=\"left\", on=\"TweetFullText\")\n",
    "retweets_df = pd.DataFrame({\n",
    "    \"UserID\": rtdf2.UserID,\n",
    "    \"TweetID\": rtdf2.TweetID,\n",
    "    \"OriginalTweetDate\": [time.strftime(\"%Y-%m-%d\", time.strptime(tweet[\"created_at\"],'%a %b %d %H:%M:%S +0000 %Y')) for tweet in qrts]\n",
    "})\n",
    "retweets_df = retweets_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>TweetID</th>\n",
       "      <th>OriginalTweetDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>992</td>\n",
       "      <td>1132049485088088065</td>\n",
       "      <td>2019-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107</td>\n",
       "      <td>1131305477449629697</td>\n",
       "      <td>2019-05-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>565</td>\n",
       "      <td>1131942587240206336</td>\n",
       "      <td>2019-05-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>749</td>\n",
       "      <td>1129442151186784256</td>\n",
       "      <td>2019-05-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>1130890140119969793</td>\n",
       "      <td>2019-05-21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID              TweetID OriginalTweetDate\n",
       "0     992  1132049485088088065        2019-05-24\n",
       "1     107  1131305477449629697        2019-05-22\n",
       "2     565  1131942587240206336        2019-05-24\n",
       "3     749  1129442151186784256        2019-05-17\n",
       "4      53  1130890140119969793        2019-05-21"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_df.to_sql(\"retweet_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"ALTER TABLE retweet_t ADD CONSTRAINT Retweet_PK PRIMARY KEY(UserID, TweetID, OriginalTweetDate)\")\n",
    "conn.execute(\"ALTER TABLE retweet_t ADD CONSTRAINT Retweet_FK_1 FOREIGN KEY(UserID) REFERENCES Twitter_User_T(UserID)\")\n",
    "conn.execute(\"ALTER TABLE retweet_t ADD CONSTRAINT Retweet_FK_2 FOREIGN KEY(TweetID) REFERENCES Tweet_T(TweetID)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_all_topics = [(kv[0], keyword) for kv in topics_keywords.items() for keyword in kv[1]]\n",
    "keywords_df = pd.DataFrame({\n",
    "    \"TopicNum\": [l[0] for t in keywords_all_topics for l in topics_df.values if t[0] == l[1]],\n",
    "    \"Keyword\": [t[1] for t in keywords_all_topics]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopicNum</th>\n",
       "      <th>Keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>immigration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>immigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>border</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>migrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>asylum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TopicNum      Keyword\n",
       "0         0  immigration\n",
       "1         0    immigrant\n",
       "2         0       border\n",
       "3         0      migrant\n",
       "4         0       asylum"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_df.to_sql(\"keyword_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"ALTER TABLE keyword_t ADD CONSTRAINT Keyword_PK PRIMARY KEY(TopicNum, Keyword)\")\n",
    "conn.execute(\"ALTER TABLE keyword_t ADD CONSTRAINT Keyword_FK FOREIGN KEY (TopicNum) REFERENCES Topic_T(TopicNum)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headline_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_df = pd.DataFrame({\n",
    "    \"ArticleID\": [i for i, article in enumerate(allarticles)],\n",
    "    \"ArticleHeadline\": nyt_headlines,\n",
    "    \"ArticleDescription\": nyt_descriptions,\n",
    "    \"DatePublished\": [datetime.strptime(obj[\"pub_date\"], \"%Y-%m-%dT%H:%M:%S%z\").strftime(\"%Y-%m-%d\") for obj in allarticles],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleID</th>\n",
       "      <th>ArticleHeadline</th>\n",
       "      <th>ArticleDescription</th>\n",
       "      <th>DatePublished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>georgia governor’s race is hurtling toward ele...</td>\n",
       "      <td>brian kemp and stacey abrams are both trying t...</td>\n",
       "      <td>2018-11-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sign up for live updates from our reporters</td>\n",
       "      <td>you'll know as soon as we do the direction the...</td>\n",
       "      <td>2018-11-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>your top 12 questions about the midterms</td>\n",
       "      <td>our political reporter alex burns answers read...</td>\n",
       "      <td>2018-11-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>bad weather will greet voters from florida to ...</td>\n",
       "      <td>rain on election day can decrease turnout, whi...</td>\n",
       "      <td>2018-11-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>on politics: election day is here</td>\n",
       "      <td>voters across the united states will choose ca...</td>\n",
       "      <td>2018-11-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleID                                    ArticleHeadline  \\\n",
       "0          0  georgia governor’s race is hurtling toward ele...   \n",
       "1          1        sign up for live updates from our reporters   \n",
       "2          2           your top 12 questions about the midterms   \n",
       "3          3  bad weather will greet voters from florida to ...   \n",
       "4          4                  on politics: election day is here   \n",
       "\n",
       "                                  ArticleDescription DatePublished  \n",
       "0  brian kemp and stacey abrams are both trying t...    2018-11-06  \n",
       "1  you'll know as soon as we do the direction the...    2018-11-06  \n",
       "2  our political reporter alex burns answers read...    2018-11-06  \n",
       "3  rain on election day can decrease turnout, whi...    2018-11-06  \n",
       "4  voters across the united states will choose ca...    2018-11-06  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_df.to_sql(\"headline_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"ALTER TABLE headline_t ADD CONSTRAINT Headline_PK PRIMARY KEY(ArticleID)\")\n",
    "conn.execute(\"ALTER TABLE headline_t ADD CONSTRAINT Headline_FK_1 FOREIGN KEY(NewsID) REFERENCES NewsSource_T(NewsID)\")\n",
    "conn.execute(\"ALTER TABLE headline_t ADD CONSTRAINT Headline_FK_2 FOREIGN KEY(HeadlineDominantTopic) REFERENCES Topic_T(TopicNum)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGram_Token_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnlp = tweets_nlp+articles_nlp\n",
    "alltokens = [token for doc in allnlp for token in doc]\n",
    "alltokens_textpos = [(token.text, token.pos_) for doc in allnlp for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16645\n"
     ]
    }
   ],
   "source": [
    "unique_tokens_text = set(alltokens_textpos)\n",
    "unique_tokens_list = list(unique_tokens_text)\n",
    "unique_tokens = []\n",
    "for token in alltokens:\n",
    "    if (token.text, token.pos_) in unique_tokens_list:\n",
    "        unique_tokens.append(token)\n",
    "        unique_tokens_list.remove((token.text, token.pos_))\n",
    "print(len(unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramtoken_df = pd.DataFrame({\n",
    "    \"TokenID\": [i for i, token in enumerate(unique_tokens)],\n",
    "    \"TokenText\": [token.text for token in unique_tokens],\n",
    "    \"PartOfSpeech\": [token.pos_ for token in unique_tokens],\n",
    "    \"TokenLemma\": [token.lemma_ for token in unique_tokens]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TokenID</th>\n",
       "      <th>TokenText</th>\n",
       "      <th>PartOfSpeech</th>\n",
       "      <th>TokenLemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>russbarnes</td>\n",
       "      <td>VERB</td>\n",
       "      <td>russbarn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>clearly</td>\n",
       "      <td>ADV</td>\n",
       "      <td>clearly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i</td>\n",
       "      <td>PRON</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>crack</td>\n",
       "      <td>VERB</td>\n",
       "      <td>crack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>myself</td>\n",
       "      <td>PRON</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TokenID   TokenText PartOfSpeech TokenLemma\n",
       "0        0  russbarnes         VERB   russbarn\n",
       "1        1     clearly          ADV    clearly\n",
       "2        2           i         PRON          i\n",
       "3        3       crack         VERB      crack\n",
       "4        4      myself         PRON     -PRON-"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramtoken_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramtoken_df.to_sql(\"ngramtoken_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"ALTER TABLE ngramtoken_t ADD CONSTRAINT NGramToken_PK PRIMARY KEY(TokenID)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TokenInTweet_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_and_nlp = list(zip(tweets_combined, tweets_nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenintweet = [(token.text, token.pos_, item[0][\"id_str\"]) for item in tweet_and_nlp for token in item[1] if not token.is_punct and not token.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokentweetseries = pd.DataFrame({\n",
    "    \"TokenText\": [tup[0] for tup in tokenintweet],\n",
    "    \"PartOfSpeech\": [tup[1] for tup in tokenintweet],\n",
    "    \"TweetID\": [tup[2] for tup in tokenintweet]\n",
    "})\n",
    "tokentweetdf1 = tokentweetseries.merge(ngramtoken_df, how='left', on=['TokenText', 'PartOfSpeech'])\n",
    "tokenintweet_df = pd.DataFrame({\n",
    "    \"TweetID\": tokentweetdf1.TweetID,\n",
    "    \"TokenID\": tokentweetdf1.TokenID\n",
    "})\n",
    "tokenintweet_df = tokenintweet_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetID</th>\n",
       "      <th>TokenID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1132456883116285953</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1132456883116285953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1132456883116285953</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1132456883116285953</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1132456883116285953</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TweetID  TokenID\n",
       "0  1132456883116285953        0\n",
       "1  1132456883116285953        1\n",
       "2  1132456883116285953        2\n",
       "3  1132456883116285953        3\n",
       "4  1132456883116285953        4"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenintweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenintweet_df.to_sql(\"tokenintweet_t\", engine, if_exists=\"append\", index=False, chunksize=200, method=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(\"ALTER TABLE tokenintweet_t ADD CONSTRAINT TokenInTweet_PK PRIMARY KEY(TokenID, TweetID)\")\n",
    "conn.execute(\"ALTER TABLE tokenintweet_t ADD CONSTRAINT TokenInTweet_FK_1 FOREIGN KEY(TokenID) REFERENCES NGramToken_T(TokenID)\")\n",
    "conn.execute(\"ALTER TABLE tokenintweet_t ADD CONSTRAINT TokenInTweet_FK_2 FOREIGN KEY(TweetID) REFERENCES Tweet_T(TweetID)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashtag_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tags len:  1844\n",
      "unique tags len:  698\n"
     ]
    }
   ],
   "source": [
    "allhashtags = [tweet[\"entities\"][\"hashtags\"] for tweet in tweets_combined if tweet[\"entities\"][\"hashtags\"] != []]\n",
    "tagtexts = [d[\"text\"].lower() for l in allhashtags for d in l]\n",
    "tagtexts_unique = list(set(tagtexts))\n",
    "print(\"all tags len: \", len(tagtexts))\n",
    "print(\"unique tags len: \", len(tagtexts_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_df = pd.DataFrame({\n",
    "    \"HashtagID\": [i for i, tag in enumerate(tagtexts_unique)],\n",
    "    \"HashtagName\": tagtexts_unique\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HashtagID</th>\n",
       "      <th>HashtagName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>moleg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>gobows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>americanfamilyact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>repealhyde</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HashtagID        HashtagName\n",
       "0          0              moleg\n",
       "1          1            florida\n",
       "2          2             gobows\n",
       "3          3  americanfamilyact\n",
       "4          4         repealhyde"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_df.to_sql(\"hashtag_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn.execute(\"ALTER TABLE hashtag_t ADD CONSTRAINT Hashtag_PK PRIMARY KEY(HashtagID)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = [tweet[\"entities\"][\"urls\"] for tweet in tweets_combined if tweet[\"entities\"][\"urls\"] != []]\n",
    "links = [d[\"display_url\"] for l in all_links for d in l]\n",
    "link_df = pd.DataFrame({\n",
    "    \"LinkID\": [i for i, l in enumerate(links)],\n",
    "    \"LinkURL\": links\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinkID</th>\n",
       "      <th>LinkURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>coryb.kr/2X0FVfj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>coryb.kr/2JxzuNJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>mobilize.us/bookerforiowa/…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>nytimes.com/2019/05/23/opi…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>coryb.kr/2JxzuNJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LinkID                      LinkURL\n",
       "0       0             coryb.kr/2X0FVfj\n",
       "1       1             coryb.kr/2JxzuNJ\n",
       "2       2  mobilize.us/bookerforiowa/…\n",
       "3       3  nytimes.com/2019/05/23/opi…\n",
       "4       4             coryb.kr/2JxzuNJ"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_df.to_sql(\"link_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UserMention_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentiontweets = [tweet for tweet in tweets_combined if tweet[\"entities\"][\"user_mentions\"] != []]\n",
    "usermentions = [tweet[\"entities\"][\"user_mentions\"] for tweet in tweets_combined if tweet[\"entities\"][\"user_mentions\"] != []]\n",
    "tweets_mentioning_candidates = []\n",
    "for tweet in tweets_combined:\n",
    "    if tweet[\"entities\"][\"user_mentions\"] != []:\n",
    "        for d in tweet[\"entities\"][\"user_mentions\"]:\n",
    "                if d[\"screen_name\"] in allhandles:\n",
    "                    tweets_mentioning_candidates.append(tweet)\n",
    "which_candidate_mentioned = [d[\"screen_name\"] for l in usermentions for d in l if d[\"screen_name\"] in allhandles]\n",
    "mentionids = [(i, tweet) for i, tweet in enumerate(tweets_combined) if tweet in tweets_mentioning_candidates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "umdf0 = pd.DataFrame({\n",
    "    \"TweetFullText\": [tweet[\"full_text\"] for tweet in tweets_mentioning_candidates]\n",
    "})\n",
    "umdf2 = umdf0.merge(tweetsdf2, how=\"left\", on=\"TweetFullText\")\n",
    "usermentiondf1 = pd.DataFrame({\n",
    "    \"TweetFullText\": [tweet[\"full_text\"] for tweet in tweets_mentioning_candidates],\n",
    "    \"UID\": umdf2.UserID,\n",
    "    \"TwitterHandle\": which_candidate_mentioned\n",
    "})\n",
    "usermdf15 = usermentiondf1.merge(cdf2, how=\"left\", on=\"TwitterHandle\")\n",
    "usermdf2 = usermdf15.merge(tweetsdf2, how=\"left\", on=\"TweetFullText\")\n",
    "usermention_df = pd.DataFrame({\n",
    "    \"UserID\": usermdf2.UserID_y,\n",
    "    \"CandidateID\": usermdf2.UserID_x,\n",
    "    \"TweetID\": usermdf2.TweetID\n",
    "})\n",
    "usermention_df = usermention_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>CandidateID</th>\n",
       "      <th>TweetID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>699</td>\n",
       "      <td>425</td>\n",
       "      <td>1132036545672884224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>306</td>\n",
       "      <td>115</td>\n",
       "      <td>1122285523786260480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>306</td>\n",
       "      <td>404</td>\n",
       "      <td>1122285523786260480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>306</td>\n",
       "      <td>690</td>\n",
       "      <td>1122285523786260480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306</td>\n",
       "      <td>425</td>\n",
       "      <td>1115239116948377600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  CandidateID              TweetID\n",
       "0     699          425  1132036545672884224\n",
       "1     306          115  1122285523786260480\n",
       "2     306          404  1122285523786260480\n",
       "3     306          690  1122285523786260480\n",
       "4     306          425  1115239116948377600"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usermention_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "usermention_df.to_sql(\"user_mention_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArticleAboutCandidate_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_candidate_names(string_contains_name):\n",
    "    for name in candidate_ln:\n",
    "        if name in string_contains_name:\n",
    "            return name\n",
    "\n",
    "candidate_names = [\"Booker, Cory\", \"Buttigieg, P\", \"Castro, J\", \"Delaney, J\", \"Gabbard, T\", \"Gillibrand\", \"Harris, Kamala\", \"Hickenlooper, J\", \"Inslee\", \"Klobuchar\", \"O'Rourke, Beto\", \"Warren, Elizabeth\", \"Williamson, Marianne\", \"Yang, Andrew\", \"Trump, Donald\", \"Sanders, B\", \"Ryan, Tim\", \"Gravel, M\", \"Swalwell, E\", \"Messam, Wayne\", \"Moulton, Seth\", \"Weld, William\", \"Biden, J\", \"de Blasio, B\", \"Bennet, Michael\", \"Bullock, Steve\"]\n",
    "articles_about_candidates = []\n",
    "for article in allarticles:\n",
    "    for d in article[\"keywords\"]:\n",
    "        for name in candidate_names:\n",
    "            if name in d[\"value\"]:\n",
    "                articles_about_candidates.append(article[\"headline\"][\"main\"].lower())\n",
    "\n",
    "candidate_in_article = []\n",
    "for article in allarticles:\n",
    "    for d in article[\"keywords\"]:\n",
    "        for name in candidate_names:\n",
    "            if name in d[\"value\"]:\n",
    "                candidate_in_article.append(d[\"value\"])\n",
    "\n",
    "candidate_articles_df1 = pd.DataFrame({\n",
    "    \"ArticleHeadline\": articles_about_candidates,\n",
    "})\n",
    "cadf2 = candidate_articles_df1.merge(headline_df, how=\"left\", on=\"ArticleHeadline\")\n",
    "cadf3 = pd.DataFrame({\n",
    "    \"ArticleID\": cadf2.ArticleID,\n",
    "    \"CandidateLastName\": candidate_in_article,\n",
    "})\n",
    "namelist = [replace_candidate_names(s) for s in cadf3.CandidateLastName]\n",
    "cadf3[\"CandidateLastName\"] = namelist\n",
    "cadf4 = cadf3.merge(candidates_df, how=\"left\", on=\"CandidateLastName\")\n",
    "article_about_candidate_df = pd.DataFrame({\n",
    "    \"ArticleID\": cadf2.ArticleID,\n",
    "    \"CandidateID\": cadf4.CandidateID\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleID</th>\n",
       "      <th>CandidateID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleID  CandidateID\n",
       "0          6          425\n",
       "1          8          425\n",
       "2          9          635\n",
       "3          9          425\n",
       "4         70          425"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_about_candidate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_about_candidate_df.to_sql(\"articleaboutcandidate_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashtagInTweet_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ht = []\n",
    "for tweet in tweets_combined:\n",
    "    if tweet[\"entities\"][\"hashtags\"] != []:\n",
    "        for d in tweet[\"entities\"][\"hashtags\"]:\n",
    "                text_ht.append((d[\"text\"].lower(), tweet[\"full_text\"]))\n",
    "htdf1 = pd.DataFrame({\n",
    "    \"TweetFullText\": [tup[1] for tup in text_ht],\n",
    "    \"HashtagName\": [tup[0] for tup in text_ht]\n",
    "})\n",
    "htdf2 = htdf1.merge(hashtag_df, how=\"left\", on=\"HashtagName\")\n",
    "htdf3 = htdf2.merge(tweetsdf2, how=\"left\", on=\"TweetFullText\")\n",
    "hashtag_in_tweet_df = pd.DataFrame({\n",
    "    \"HashtagID\": htdf3.HashtagID,\n",
    "    \"TweetID\": htdf3.TweetID\n",
    "})\n",
    "hashtag_in_tweet_df = hashtag_in_tweet_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HashtagID</th>\n",
       "      <th>TweetID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>1131569358587727872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>325</td>\n",
       "      <td>1131318535811162112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>398</td>\n",
       "      <td>1130922527285567488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>681</td>\n",
       "      <td>1127622062309355521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>641</td>\n",
       "      <td>1126479138838257665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HashtagID              TweetID\n",
       "0        485  1131569358587727872\n",
       "1        325  1131318535811162112\n",
       "2        398  1130922527285567488\n",
       "3        681  1127622062309355521\n",
       "4        641  1126479138838257665"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_in_tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_in_tweet_df.to_sql(\"hashtagintweet_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TokenInHeadline_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeninheadline = [(i, token.text) for i, article in enumerate(articles_nlp) for token in article]\n",
    "tokenhead_df1 = pd.DataFrame({\n",
    "    \"ArticleID\": [i for i, t in tokeninheadline],\n",
    "    \"TokenText\": [t for i, t in tokeninheadline]\n",
    "})\n",
    "tokenhead_df2 = tokenhead_df1.merge(ngramtoken_df, how=\"left\", on=\"TokenText\")\n",
    "token_in_headline_df = pd.DataFrame({\n",
    "    \"TokenID\": tokenhead_df2.TokenID,\n",
    "    \"ArticleID\": tokenhead_df2.ArticleID\n",
    "})\n",
    "token_in_headline_df = token_in_headline_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TokenID</th>\n",
       "      <th>ArticleID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14860</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14926</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TokenID  ArticleID\n",
       "0       86          0\n",
       "1     4324          0\n",
       "2     4485          0\n",
       "3    14860          0\n",
       "4    14926          0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_in_headline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_in_headline_df.to_sql(\"tokeninheadline_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinkInstanceTweet_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_with_links = []\n",
    "for tweet in tweets_combined:\n",
    "    if tweet[\"entities\"][\"urls\"] != []:\n",
    "        for d in tweet[\"entities\"][\"urls\"]:\n",
    "            tweets_with_links.append(tweet)\n",
    "linktweetdf1 = pd.DataFrame({\n",
    "    \"LinkID\": link_df.LinkID,\n",
    "    \"TweetFullText\": [tweet[\"full_text\"] for tweet in tweets_with_links]\n",
    "})\n",
    "linktweetdf2 = linktweetdf1.merge(tweetsdf2, how=\"left\", on=\"TweetFullText\")\n",
    "linkinstancetweet_df = pd.DataFrame({\n",
    "    \"LinkID\": link_df.LinkID,\n",
    "    \"TweetID\": linktweetdf2.TweetID\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinkID</th>\n",
       "      <th>TweetID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1132332387763937280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1132045252620169216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1132020165766451205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1131617526490783744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1131579847023779840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LinkID              TweetID\n",
       "0       0  1132332387763937280\n",
       "1       1  1132045252620169216\n",
       "2       2  1132020165766451205\n",
       "3       3  1131617526490783744\n",
       "4       4  1131579847023779840"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkinstancetweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkinstancetweet_df.to_sql(\"linkinstancetweet_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopicInTweet_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_by_topic\n",
    "tweet_topic_pairs = [(kv[0], tweet[\"full_text\"]) for kv in tweets_by_topic.items() for tweet in kv[1]]\n",
    "tt_df1 = pd.DataFrame({\n",
    "    \"TopicNum\": [l[0] for t in tweet_topic_pairs for l in topics_df.values if l[1] == t[0]],\n",
    "    \"TweetFullText\": [t[1] for t in tweet_topic_pairs]\n",
    "})\n",
    "\n",
    "tt_df2 = tt_df1.merge(tweetsdf2, how=\"left\", on=\"TweetFullText\")\n",
    "topicintweet_df = pd.DataFrame({\n",
    "    \"TopicNum\": tt_df2.TopicNum,\n",
    "    \"TweetID\": tt_df2.TweetID\n",
    "})\n",
    "topicintweet_df = topicintweet_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopicNum</th>\n",
       "      <th>TweetID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1129127173598711809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1127258994513522688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1127257199892471810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1125938417341280256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1123756751306809347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TopicNum              TweetID\n",
       "0         0  1129127173598711809\n",
       "2         0  1127258994513522688\n",
       "3         0  1127257199892471810\n",
       "4         0  1125938417341280256\n",
       "6         0  1123756751306809347"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicintweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicintweet_df.to_sql(\"topicintweet_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopicInHeadline_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_topic_pairs = [(kv[0], headline) for kv in articles_by_topic.items() for headline in kv[1]]\n",
    "htopic_df1 = pd.DataFrame({\n",
    "    \"TopicNum\": [l[0] for t in article_topic_pairs for l in topics_df.values if l[1] == t[0]],\n",
    "    \"ArticleHeadline\": [t[1] for t in article_topic_pairs]\n",
    "})\n",
    "htopic_df2 = htopic_df1.merge(headline_df, how=\"left\", on=\"ArticleHeadline\")\n",
    "topicinheadline_df = pd.DataFrame({\n",
    "    \"TopicNum\": htopic_df2.TopicNum,\n",
    "    \"ArticleID\": htopic_df2.ArticleID\n",
    "})\n",
    "topicinheadline_df = topicinheadline_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopicNum</th>\n",
       "      <th>ArticleID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TopicNum  ArticleID\n",
       "0         0        137\n",
       "1         0        162\n",
       "2         0        267\n",
       "3         0        268\n",
       "5         0        277"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicinheadline_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicinheadline_df.to_sql(\"topicinheadline_t\", engine, if_exists=\"append\", index=False, method=\"multi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
